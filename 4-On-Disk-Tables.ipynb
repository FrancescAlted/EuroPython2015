{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Querying Out-of-Core datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * Compare queries of tabular data for **on-disk** containers\n",
    "> * Compare sizes and times for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [1] used 0.105 MiB RAM in 0.002s, peaked 0.000 MiB above current, total RAM usage 27.977 MiB\n"
     ]
    }
   ],
   "source": [
    "from ipython_memwatcher import MemWatcher\n",
    "mw = MemWatcher()\n",
    "mw.start_watching_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load movielens datasets in pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [2] used 0.031 MiB RAM in 0.003s, peaked 0.000 MiB above current, total RAM usage 28.008 MiB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dset = 'movielens-1m'\n",
    "fdata = os.path.join(dset, 'ratings.dat.gz')\n",
    "fitem = os.path.join(dset, 'movies.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [3] used 109.078 MiB RAM in 3.673s, peaked 0.000 MiB above current, total RAM usage 137.086 MiB\n"
     ]
    }
   ],
   "source": [
    "# Import CSV files via pandas\n",
    "import pandas as pd\n",
    "# pass in column names for each CSV\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(fdata, sep=';', names=r_cols, compression='gzip')\n",
    "\n",
    "m_cols = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_csv(fitem, sep=';', names=m_cols,\n",
    "                     dtype={'title': object, 'genres': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [4] used -0.375 MiB RAM in 20.833s, peaked 247.270 MiB above current, total RAM usage 136.711 MiB\n"
     ]
    }
   ],
   "source": [
    "# Store movies and ratings in 2 separate tables in SQLite\n",
    "sqlite_norm = \"movielens-norm.sqlite\"\n",
    "if os.path.exists(sqlite_norm):\n",
    "    os.unlink(sqlite_norm)\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(sqlite_norm)\n",
    "conn.text_factory = str   # Shut up problems with Unicode\n",
    "ratings.to_sql(\"ratings\", conn)\n",
    "movies.to_sql(\"movies\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [5] used 92.551 MiB RAM in 0.475s, peaked 0.000 MiB above current, total RAM usage 229.262 MiB\n"
     ]
    }
   ],
   "source": [
    "# create one merged DataFrame\n",
    "lens = pd.merge(movies, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000209 entries, 0 to 1000208\n",
      "Data columns (total 6 columns):\n",
      "movie_id          1000209 non-null int64\n",
      "title             1000209 non-null object\n",
      "genres            1000209 non-null object\n",
      "user_id           1000209 non-null int64\n",
      "rating            1000209 non-null int64\n",
      "unix_timestamp    1000209 non-null int64\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 53.4+ MB\n",
      "In [6] used 37.047 MiB RAM in 0.597s, peaked 0.176 MiB above current, total RAM usage 266.309 MiB\n"
     ]
    }
   ],
   "source": [
    "lens.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [7] used -60.887 MiB RAM in 24.549s, peaked 306.523 MiB above current, total RAM usage 205.422 MiB\n"
     ]
    }
   ],
   "source": [
    "# Store lens denormalized in 1 table in SQLite\n",
    "sqlite_denorm = \"movielens-denorm.sqlite\"\n",
    "if os.path.exists(sqlite_denorm):\n",
    "    os.unlink(sqlite_denorm)\n",
    "conn_denorm = sqlite3.connect(sqlite_denorm)\n",
    "conn_denorm.text_factory = str   # Shut up problems with Unicode\n",
    "lens.to_sql(\"lens\", conn_denorm)\n",
    "conn_denorm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [8] used 0.004 MiB RAM in 0.001s, peaked 0.000 MiB above current, total RAM usage 205.426 MiB\n"
     ]
    }
   ],
   "source": [
    "del movies, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 131 ms, sys: 11.6 ms, total: 143 ms\n",
      "Wall time: 196 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5121      75\n",
       "5164    3842\n",
       "5187    6031\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [9] used 9.223 MiB RAM in 0.215s, peaked 0.000 MiB above current, total RAM usage 214.648 MiB\n"
     ]
    }
   ],
   "source": [
    "%time result = lens.query(\"(title == 'Tom and Huck (1995)') & (rating == 5)\")['user_id']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5121      75\n",
       "5164    3842\n",
       "5187    6031\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [10] used 0.004 MiB RAM in 0.005s, peaked 0.000 MiB above current, total RAM usage 214.652 MiB\n"
     ]
    }
   ],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pandas_mem = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying on-disk data with SQLite (relational database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [11] used 0.004 MiB RAM in 0.002s, peaked 0.000 MiB above current, total RAM usage 214.656 MiB\n"
     ]
    }
   ],
   "source": [
    "# Execute the query with the de-normalized SQLite database\n",
    "conn = sqlite3.connect(sqlite_denorm)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 626 ms, sys: 304 ms, total: 930 ms\n",
      "Wall time: 1.89 s\n",
      "In [12] used 0.094 MiB RAM in 1.892s, peaked 0.000 MiB above current, total RAM usage 214.750 MiB\n"
     ]
    }
   ],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "%time result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(75,), (3842,), (6031,)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [13] used 0.016 MiB RAM in 0.004s, peaked 0.000 MiB above current, total RAM usage 214.766 MiB\n"
     ]
    }
   ],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_denorm = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [14] used 0.012 MiB RAM in 0.005s, peaked 0.000 MiB above current, total RAM usage 214.777 MiB\n"
     ]
    }
   ],
   "source": [
    "# Execute the query with the normalized SQLite database\n",
    "conn = sqlite3.connect(sqlite_norm)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 5.48 s, total: 17 s\n",
      "Wall time: 19.3 s\n",
      "In [15] used 1.406 MiB RAM in 19.304s, peaked 0.945 MiB above current, total RAM usage 216.184 MiB\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\"\n",
    "select ratings.user_id from movies \n",
    "INNER JOIN ratings ON movies.movie_id = ratings.movie_id\n",
    "where movies.title == 'Tom and Huck (1995)' and ratings.rating == 5\n",
    "\"\"\"\n",
    "%time result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(75,), (3842,), (6031,)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In [16] used -3.621 MiB RAM in 0.004s, peaked 0.000 MiB above current, total RAM usage 212.562 MiB\n"
     ]
    }
   ],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_norm = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in general, we see that it is much faster to query tables in denormalized form, although they take much more storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158368 movielens-denorm.sqlite  75448 movielens-norm.sqlite\n",
      "In [17] used 0.008 MiB RAM in 0.061s, peaked 0.000 MiB above current, total RAM usage 212.570 MiB\n"
     ]
    }
   ],
   "source": [
    "!ls -sh $sqlite_norm $sqlite_denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some way, storing tables in normalized form is a kind of compression, but that comes to the cost of using more time to process queries.  Let's enter another way to compress denormalized data tables on-disk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bcolz` cannot only deal with data in-memory, but also on-disk exactly in the same way.  Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "bcolz version:     0.9.0\n",
      "NumPy version:     1.9.2\n",
      "Blosc version:     1.4.1 ($Date:: 2014-07-08 #$)\n",
      "Blosc compressors: ['blosclz', 'lz4', 'lz4hc', 'snappy', 'zlib']\n",
      "Numexpr version:   2.4.3\n",
      "Python version:    3.4.3 |Anaconda 2.3.0 (x86_64)| (default, Mar  6 2015, 12:07:41) \n",
      "[GCC 4.2.1 (Apple Inc. build 5577)]\n",
      "Platform:          darwin-x86_64\n",
      "Byte-ordering:     little\n",
      "Detected cores:    2\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "In [18] used -30.609 MiB RAM in 0.564s, peaked 0.000 MiB above current, total RAM usage 181.961 MiB\n"
     ]
    }
   ],
   "source": [
    "import bcolz\n",
    "bcolz.print_versions()\n",
    "bcolz.defaults.cparams['cname'] = 'lz4'\n",
    "bcolz.defaults.cparams['clevel'] = 5\n",
    "# bcolz.set_nthreads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import a pandas DataFrame in a ctable on-disk\n",
    "bcolz_dir = \"movielens-denorm.bcolz\"\n",
    "if os.path.exists(bcolz_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(bcolz_dir)\n",
    "zlens = bcolz.ctable.fromdataframe(lens, rootdir=bcolz_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = zlens[\"(title == b'Tom and Huck (1995)') & (rating == 5)\"]['user_id']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_bcolz = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One can optimize the query more\n",
    "%time result = [r.user_id for r in zlens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\", outcols=['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_bcolz_opt = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the space consumed on-disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!du -sh $bcolz_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's compare it with the internal estimated size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the internal estimated size is a bit higher than the actual size on-disk, which is pretty good.\n",
    "\n",
    "Another interesting fact is that the de-normalized version of the ctable takes less space than the normalized version in a relational database, and the query speed is still faster (than SQLite at least).\n",
    "\n",
    "But there are other ways to store compressed tables.  Let's visit PyTables and how it performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5denorm = \"movielens-denorm.h5\"\n",
    "if os.path.exists(h5denorm):\n",
    "    os.unlink(h5denorm)\n",
    "zlens.tohdf5(h5denorm, nodepath='/h5lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables\n",
    "h5file = tables.open_file(h5denorm)\n",
    "h5lens = h5file.root.h5lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the HDF5 table\n",
    "h5lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, it seems that PyTables has a similar performance than SQLite for the queries (denormalized case).  What about the size of the HDF5 file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5file.close()\n",
    "!ls -sh $h5denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 MB is very close to 6.6 MB that used bcolz (which is expected because both are using LZ4 as the compressor), and much less than pandas and SQLite.\n",
    "\n",
    "Now, pandas comes with its own interface to PyTables via the HDFStore sub-package.  Let's check that out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFStore (pandas HDF5 via PyTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5pandas = \"movielens-pandas.h5\"\n",
    "if os.path.exists(h5pandas):\n",
    "    os.unlink(h5pandas)\n",
    "from pandas import HDFStore\n",
    "hdf = HDFStore(h5pandas, complevel=5, complib=\"blosc\")\n",
    "hdf.put('h5lens', lens, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf['h5lens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time hdf.select('h5lens', where=\"(title == 'Tom and Huck (1995)') & (rating == 5)\", columns=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pandas_hdf5 = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the time is very close to the time that it takes the query on a pandas DataFrame in memory.  What about the size on-disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -sh $h5pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so 13 MB is around 80% larger than a pure PyTables file with the same info (7.1 MB).  Where the overhead comes from?  Well, it turns out that pandas makes use of the indexing capabilities of PyTables automatically, so let's make PyTables to index the columns that participate in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing (PyTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Copy the original PyTables table into another file\n",
    "import shutil\n",
    "h5idx = \"movielens-indexed.h5\"\n",
    "if os.path.exists(h5idx):\n",
    "    os.unlink(h5idx)\n",
    "shutil.copyfile(h5denorm, h5idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the new file in 'a'ppend mode\n",
    "h5i = tables.open_file(h5idx, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an index for the 'title' column\n",
    "h5lens = h5i.root.h5lens\n",
    "h5lens.cols.title.create_csindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...and redo the query...\n",
    "t = %timeit -r1 -n1 -o result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables_index1 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, by indexing one column we have got a 30x of acceleration wrt PyTables (11 vs 350 ms).  What happens if we index the 'rating' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5lens.cols.rating.create_csindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = %timeit -r1 -n1 -o result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables_index2 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's another ~5x additional acceleration, and the best time that we ever reached for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5i.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Indexing (SQLite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are of course curious to find out how indexing in relational databases fares against PyTables' own indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlite_idx = \"movielens-indexed.sqlite\"\n",
    "if os.path.exists(sqlite_idx):\n",
    "    os.unlink(sqlite_idx)\n",
    "shutil.copyfile(sqlite_denorm, sqlite_idx)\n",
    "conn_idx = sqlite3.connect(sqlite_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = conn_idx.cursor()\n",
    "c.execute(\"CREATE INDEX index_title ON lens (title)\")\n",
    "conn_idx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "t = %timeit -r1 -n1 -o result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_index1 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, with ~1 ms we are getting the best figure so far, and faster than the best figure with indexed PyTables.  Now, what will happen with indexing the second column?  We should get better speed, right?  Wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.execute(\"CREATE INDEX index_ratings ON lens (rating)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "t = %timeit -r1 -n1 -o results = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_index2 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a not pleasant surprise: adding another index is decrasing the speed by a factor of 170x (!).\n",
    "\n",
    "The leason to learn here is that indexing is tricky and you should always double check whether a new indexing operation is going to be beneficial for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn_idx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query times\n",
    "labels = [\"pandas (in-memory)\", \"bcolz\", \"PyTables\", \"SQLite denorm\", \"pandas w/ PyTables (indexed)\", \"PyTables (indexed)\", \"SQLite (indexed)\"]\n",
    "df = pd.DataFrame({'time (sec)': [qtime_pandas_mem, qtime_bcolz_opt, qtime_pytables, qtime_sqlite_denorm, qtime_pandas_hdf5, qtime_pytables_index2, qtime_sqlite_index1]}, index=labels)\n",
    "pd.options.display.mpl_style = 'default'\n",
    "df.plot(kind='barh', figsize=(12,5), fontsize=16, title=\"Query times for MovieLens 1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!du -sh movielens* | sort -nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Container sizes\n",
    "df = pd.DataFrame({'size (MB)': [53, 6.6, 7.1, 78, 13, 11, 119]}, index=labels)\n",
    "pd.options.display.mpl_style = 'default'\n",
    "df.plot(kind='barh', figsize=(12,5), fontsize=16, title=\"Container sizes for MovieLens 1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Containers Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are many different solutions for storing large datasets both in-memory and on-disk.  Here it is a summary of the ones that we have seen in this tutorial:\n",
    "\n",
    "* In-core\n",
    "  * Python lists and dictionaries: Included in Python.  Very flexible.  Not efficient for large datasets.\n",
    "  * NumPy: Good for large amounts of data and with a lot of functionality.  Does not support compression.\n",
    "  * Pandas: Good for large amounts of data and with a lot of functionality.  Does not support compression.\n",
    "  * bcolz: Good for large amounts of data, but reduced functionality.  Supports compression.\n",
    "  \n",
    "* Out-of-core\n",
    "  * Pandas/HDFStore: Good for large datasets.  Supports indexing and compression.  Medium query speeds.\n",
    "  * PyTables: Good for large datasets.  Supports indexing and compression.  Good query speeds.\n",
    "  * bcolz: Good for large datasets.  Supports compression, but not indexing.  Medium query speeds.\n",
    "  * SQLite: Can be used for large datasets, buttakes a lot of disk.  Supports compression, but indexing.  Excellent query speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, every solution has strengths and weaknesses, with wild variations in resource consumptions, so the wise thing to do is to know them better and try to apply the best candidate to your scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
