{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Querying Out-of-Core datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Objectives:\n",
    "> * Compare queries of tabular data for **on-disk** containers\n",
    "> * Compare sizes and times for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ipython_memwatcher import MemWatcher\n",
    "mw = MemWatcher()\n",
    "mw.start_watching_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load movielens datasets in pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dset = 'movielens-1m'\n",
    "fdata = os.path.join(dset, 'ratings.dat.gz')\n",
    "fitem = os.path.join(dset, 'movies.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import CSV files via pandas\n",
    "import pandas as pd\n",
    "# pass in column names for each CSV\n",
    "r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\n",
    "ratings = pd.read_csv(fdata, sep=';', names=r_cols, compression='gzip')\n",
    "\n",
    "m_cols = ['movie_id', 'title', 'genres']\n",
    "movies = pd.read_csv(fitem, sep=';', names=m_cols,\n",
    "                     dtype={'title': object, 'genres': object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store movies and ratings in 2 separate tables in SQLite\n",
    "sqlite_norm = \"movielens-norm.sqlite\"\n",
    "if os.path.exists(sqlite_norm):\n",
    "    os.unlink(sqlite_norm)\n",
    "import sqlite3\n",
    "conn = sqlite3.connect(sqlite_norm)\n",
    "conn.text_factory = str   # Shut up problems with Unicode\n",
    "ratings.to_sql(\"ratings\", conn)\n",
    "movies.to_sql(\"movies\", conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create one merged DataFrame\n",
    "lens = pd.merge(movies, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lens.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store lens denormalized in 1 table in SQLite\n",
    "sqlite_denorm = \"movielens-denorm.sqlite\"\n",
    "if os.path.exists(sqlite_denorm):\n",
    "    os.unlink(sqlite_denorm)\n",
    "conn_denorm = sqlite3.connect(sqlite_denorm)\n",
    "conn_denorm.text_factory = str   # Shut up problems with Unicode\n",
    "lens.to_sql(\"lens\", conn_denorm)\n",
    "conn_denorm.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del movies, ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = lens.query(\"(title == 'Tom and Huck (1995)') & (rating == 5)\")['user_id']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pandas_mem = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying on-disk data with SQLite (relational database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Execute the query with the de-normalized SQLite database\n",
    "conn = sqlite3.connect(sqlite_denorm)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "%time result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_denorm = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Execute the query with the normalized SQLite database\n",
    "conn = sqlite3.connect(sqlite_norm)\n",
    "c = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "select ratings.user_id from movies \n",
    "INNER JOIN ratings ON movies.movie_id = ratings.movie_id\n",
    "where movies.title == 'Tom and Huck (1995)' and ratings.rating == 5\n",
    "\"\"\"\n",
    "%time result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_norm = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in general, we see that it is much faster to query tables in denormalized form, although they take much more storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -sh $sqlite_norm $sqlite_denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some way, storing tables in normalized form is a kind of compression, but that comes to the cost of using more time to process queries.  Let's enter another way to compress denormalized data tables on-disk:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bcolz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`bcolz` cannot only deal with data in-memory, but also on-disk exactly in the same way.  Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import bcolz\n",
    "bcolz.print_versions()\n",
    "bcolz.defaults.cparams['cname'] = 'lz4'\n",
    "bcolz.defaults.cparams['clevel'] = 5\n",
    "# bcolz.set_nthreads(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import a pandas DataFrame in a ctable on-disk\n",
    "bcolz_dir = \"movielens-denorm.bcolz\"\n",
    "if os.path.exists(bcolz_dir):\n",
    "    import shutil\n",
    "    shutil.rmtree(bcolz_dir)\n",
    "zlens = bcolz.ctable.fromdataframe(lens, rootdir=bcolz_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = zlens[\"(title == b'Tom and Huck (1995)') & (rating == 5)\"]['user_id']\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_bcolz = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One can optimize the query more\n",
    "%time result = [r.user_id for r in zlens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\", outcols=['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_bcolz_opt = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the space consumed on-disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!du -sh $bcolz_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and let's compare it with the internal estimated size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "zlens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the internal estimated size is a bit higher than the actual size on-disk, which is pretty good.\n",
    "\n",
    "Another interesting fact is that the de-normalized version of the ctable takes less space than the normalized version in a relational database, and the query speed is still faster (than SQLite at least).\n",
    "\n",
    "But there are other ways to store compressed tables.  Let's visit PyTables and how it performs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5denorm = \"movielens-denorm.h5\"\n",
    "if os.path.exists(h5denorm):\n",
    "    os.unlink(h5denorm)\n",
    "zlens.tohdf5(h5denorm, nodepath='/h5lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tables\n",
    "h5file = tables.open_file(h5denorm)\n",
    "h5lens = h5file.root.h5lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's have a look at the HDF5 table\n",
    "h5lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So, it seems that PyTables has a similar performance than SQLite for the queries (denormalized case).  What about the size of the HDF5 file?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5file.close()\n",
    "!ls -sh $h5denorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7.1 MB is very close to 6.6 MB that used bcolz (which is expected because both are using LZ4 as the compressor), and much less than pandas and SQLite.\n",
    "\n",
    "Now, pandas comes with its own interface to PyTables via the HDFStore sub-package.  Let's check that out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFStore (pandas HDF5 via PyTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5pandas = \"movielens-pandas.h5\"\n",
    "if os.path.exists(h5pandas):\n",
    "    os.unlink(h5pandas)\n",
    "from pandas import HDFStore\n",
    "hdf = HDFStore(h5pandas, complevel=5, complib=\"blosc\")\n",
    "hdf.put('h5lens', lens, format='table', data_columns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf['h5lens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time hdf.select('h5lens', where=\"(title == 'Tom and Huck (1995)') & (rating == 5)\", columns=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pandas_hdf5 = mw.measurements.time_delta\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the time is very close to the time that it takes the query on a pandas DataFrame in memory.  What about the size on-disk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -sh $h5pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so 13 MB is around 80% larger than a pure PyTables file with the same info (7.1 MB).  Where the overhead comes from?  Well, it turns out that pandas makes use of the indexing capabilities of PyTables automatically, so let's make PyTables to index the columns that participate in the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing (PyTables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Copy the original PyTables table into another file\n",
    "import shutil\n",
    "h5idx = \"movielens-indexed.h5\"\n",
    "if os.path.exists(h5idx):\n",
    "    os.unlink(h5idx)\n",
    "shutil.copyfile(h5denorm, h5idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open the new file in 'a'ppend mode\n",
    "h5i = tables.open_file(h5idx, mode=\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an index for the 'title' column\n",
    "h5lens = h5i.root.h5lens\n",
    "h5lens.cols.title.create_csindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...and redo the query...\n",
    "t = %timeit -r1 -n1 -o result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables_index1 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, by indexing one column we have got a 30x of acceleration wrt PyTables (11 vs 350 ms).  What happens if we index the 'rating' column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5lens.cols.rating.create_csindex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = %timeit -r1 -n1 -o result = [r['user_id'] for r in h5lens.where(\"(title == b'Tom and Huck (1995)') & (rating == 5)\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_pytables_index2 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that's another ~5x additional acceleration, and the best time that we ever reached for this query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h5i.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Indexing (SQLite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are of course curious to find out how indexing in relational databases fares against PyTables' own indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sqlite_idx = \"movielens-indexed.sqlite\"\n",
    "if os.path.exists(sqlite_idx):\n",
    "    os.unlink(sqlite_idx)\n",
    "shutil.copyfile(sqlite_denorm, sqlite_idx)\n",
    "conn_idx = sqlite3.connect(sqlite_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = conn_idx.cursor()\n",
    "c.execute(\"CREATE INDEX index_title ON lens (title)\")\n",
    "conn_idx.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "t = %timeit -r1 -n1 -o result = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_index1 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, with ~1 ms we are getting the best figure so far, and faster than the best figure with indexed PyTables.  Now, what will happen with indexing the second column?  We should get better speed, right?  Wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c.execute(\"CREATE INDEX index_ratings ON lens (rating)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = \"select user_id from lens where title == 'Tom and Huck (1995)' and rating == 5\"\n",
    "t = %timeit -r1 -n1 -o results = [r for r in c.execute(q)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# keep the run-time for reference\n",
    "qtime_sqlite_index2 = t.best\n",
    "# ...and print the result (for reference too)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a not pleasant surprise: adding another index is decrasing the speed by a factor of 170x (!).\n",
    "\n",
    "The leason to learn here is that indexing is tricky and you should always double check whether a new indexing operation is going to be beneficial for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn_idx.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query times\n",
    "labels = [\"pandas (in-memory)\", \"bcolz\", \"PyTables\", \"SQLite denorm\", \"pandas w/ PyTables (indexed)\", \"PyTables (indexed)\", \"SQLite (indexed)\"]\n",
    "df = pd.DataFrame({'time (sec)': [qtime_pandas_mem, qtime_bcolz_opt, qtime_pytables, qtime_sqlite_denorm, qtime_pandas_hdf5, qtime_pytables_index2, qtime_sqlite_index1]}, index=labels)\n",
    "pd.options.display.mpl_style = 'default'\n",
    "df.plot(kind='barh', figsize=(12,5), fontsize=16, title=\"Query times for MovieLens 1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final size comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!du -sh movielens* | sort -nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Container sizes\n",
    "df = pd.DataFrame({'size (MB)': [53, 6.6, 7.1, 78, 13, 11, 119]}, index=labels)\n",
    "pd.options.display.mpl_style = 'default'\n",
    "df.plot(kind='barh', figsize=(12,5), fontsize=16, title=\"Container sizes for MovieLens 1m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Containers Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, there are many different solutions for storing large datasets both in-memory and on-disk.  Here it is a summary of the ones that we have seen in this tutorial:\n",
    "\n",
    "* In-core\n",
    "  * Python lists and dictionaries: Included in Python.  Very flexible.  Not efficient for large datasets.\n",
    "  * NumPy: Good for large amounts of data and with a lot of functionality.  Does not support compression.\n",
    "  * Pandas: Good for large amounts of data and with a lot of functionality.  Does not support compression.\n",
    "  * bcolz: Good for large amounts of data, but reduced functionality.  Supports compression.\n",
    "  \n",
    "* Out-of-core\n",
    "  * Pandas/HDFStore: Good for large datasets.  Supports indexing and compression.  Medium query speeds.\n",
    "  * PyTables: Good for large datasets.  Supports indexing and compression.  Good query speeds.\n",
    "  * bcolz: Good for large datasets.  Supports compression, but not indexing.  Medium query speeds.\n",
    "  * SQLite: Can be used for large datasets, buttakes a lot of disk.  Supports compression, but indexing.  Excellent query speeds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, every solution has strengths and weaknesses, with wild variations in resource consumptions, so the wise thing to do is to know them better and try to apply the best candidate to your scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
